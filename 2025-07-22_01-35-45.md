- Today I managed to generate the reference motion for my new robot D2, which will be used to train an imitation control policy. The reference data is generated by placo and computes the inverse kinematics required to achieve walking motion based on a parameterised gait (with variables such as step size, centre of mass height, foot height at top of step etc), recording the state of the joints at every timestep. Then a polynomial is fit to parameterise each trajectory, producing a smooth target trajectory for the robot state. In the ideal case, you could play this trajectory on the robot and it would walk as done during the recording. The reality, of couse, is that the real world is more complicated. The robot will blindly follow this trajectory whether it falls over or stays up. If the robot needs to rebalance, stabilise against pertubation, or adapt in any way, its going to need to get intelligent.
- The next step is to try to replay this reference motion in mujoco so i can visualise the full robot render performing the parameterised gates. I suspect this is where the scaling issue of my URDF generation (which generates a 100ft robot due to not converting from mm to m) is going to become a problem (0.1m steps are going to look way out of proportion). To do this I need to migrate the visualisation code that works for my previous robot into the Dojo (where robots are trained).